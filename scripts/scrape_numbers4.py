"""
ãƒŠãƒ³ãƒãƒ¼ã‚º4 ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ¥½å¤©å®ãã˜ã‚µã‚¤ãƒˆã‹ã‚‰ãƒŠãƒ³ãƒãƒ¼ã‚º4ã®å½“é¸ç•ªå·ã¨é…å½“é‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

å–å¾—é …ç›®:
- å›å·ã€æŠ½ã›ã‚“æ—¥ã€å½“ã›ã‚“ç•ªå·ã€å„æ¡(digit1-4)
- ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆå½“é¸é‡‘ã€ãƒœãƒƒã‚¯ã‚¹å½“é¸é‡‘
- ã‚»ãƒƒãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆå½“é¸é‡‘ã€ã‚»ãƒƒãƒˆãƒœãƒƒã‚¯ã‚¹å½“é¸é‡‘

ä½¿ç”¨æ–¹æ³•:
  # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
  python scripts/scrape_numbers4.py

  # å¢—åˆ†æ›´æ–°
  python scripts/scrape_numbers4.py --append

  # é…å½“é‡‘æƒ…å ±ã®ã¿è£œå®Œ
  python scripts/scrape_numbers4.py --fill-payouts
"""

import re
import time
import argparse
import os
from typing import Optional, Dict, List
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import pandas as pd

BASE_URL = "https://takarakuji.rakuten.co.jp/backnumber/numbers4/"
PAST_URL = "https://takarakuji.rakuten.co.jp/backnumber/numbers4_past/"
DETAIL_BASE = "https://takarakuji.rakuten.co.jp/backnumber/numbers4_detail/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
}

# æœˆæ¬¡ãƒšãƒ¼ã‚¸ã®å¯¾è±¡æœˆã‚’ç”Ÿæˆï¼ˆ2024å¹´9æœˆï½ç¾åœ¨æœˆ+2ãƒ¶æœˆå…ˆã¾ã§ï¼‰
def generate_months() -> List[str]:
    """å¯¾è±¡ã¨ãªã‚‹æœˆã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    months = []
    start_year, start_month = 2024, 9
    now = datetime.now()
    end_year, end_month = now.year, now.month + 2  # 2ãƒ¶æœˆå…ˆã¾ã§
    
    if end_month > 12:
        end_month -= 12
        end_year += 1
    
    current_year, current_month = start_year, start_month
    while (current_year, current_month) <= (end_year, end_month):
        months.append(f"{current_year}{current_month:02d}")
        current_month += 1
        if current_month > 12:
            current_month = 1
            current_year += 1
    
    return months


MONTHS = generate_months()


def parse_payout_amount(text: str) -> Optional[int]:
    """é…å½“é‡‘ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡‘é¡ã‚’æŠ½å‡ºï¼ˆå††å˜ä½ï¼‰
    
    ä¾‹: "940,800å††" â†’ 940800
        "è©²å½“ãªã—" â†’ None
    """
    if not text or "è©²å½“ãªã—" in text:
        return None
    # ã‚«ãƒ³ãƒã¨å††ã‚’é™¤å»ã—ã¦æ•°å€¤ã‚’æŠ½å‡º
    cleaned = re.sub(r"[,å††\s]", "", text)
    match = re.search(r"(\d+)", cleaned)
    if match:
        return int(match.group(1))
    return None


def scrape_month_with_payouts(url: str, session: requests.Session) -> List[Dict]:
    """æœˆæ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰å½“é¸ç•ªå·ã¨é…å½“é‡‘æƒ…å ±ã‚’å–å¾—
    
    Args:
        url: æœˆæ¬¡ãƒšãƒ¼ã‚¸ã®URL (ä¾‹: https://takarakuji.rakuten.co.jp/backnumber/numbers4/202512/)
        session: requestsã‚»ãƒƒã‚·ãƒ§ãƒ³
    
    Returns:
        å½“é¸çµæœã®ãƒªã‚¹ãƒˆã€‚å„çµæœã¯ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’å«ã‚€è¾æ›¸:
        - å›å·, æŠ½ã›ã‚“æ—¥, å½“ã›ã‚“ç•ªå·, digit1-4
        - straight_payout, box_payout, set_straight_payout, set_box_payout
    """
    try:
        res = session.get(url, headers=HEADERS, timeout=20)
        if res.status_code != 200:
            return []
        res.encoding = "utf-8"
        soup = BeautifulSoup(res.text, "lxml")
    except Exception:
        return []

    results = []
    
    # å„å›å·ã”ã¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
    tables = soup.select("table.tblType02.tblNumberGuid")
    
    for table in tables:
        result = {
            "å›å·": None,
            "æŠ½ã›ã‚“æ—¥": None,
            "å½“ã›ã‚“ç•ªå·": None,
            "digit1": None,
            "digit2": None,
            "digit3": None,
            "digit4": None,
            "straight_payout": None,
            "box_payout": None,
            "set_straight_payout": None,
            "set_box_payout": None,
        }
        
        rows = table.select("tr")
        for row in rows:
            th = row.find("th")
            if not th:
                continue
            
            label = th.get_text(strip=True)
            tds = row.find_all("td")
            
            if label == "å›å·":
                # "ç¬¬6868å›" ã®ã‚ˆã†ãªå½¢å¼
                ths = row.find_all("th")
                if len(ths) > 1:
                    result["å›å·"] = ths[1].get_text(strip=True)
            
            elif label == "æŠ½ã›ã‚“æ—¥" and tds:
                result["æŠ½ã›ã‚“æ—¥"] = tds[0].get_text(strip=True)
            
            elif label == "å½“ã›ã‚“ç•ªå·" and tds:
                numbers = tds[0].get_text(strip=True).strip()
                num_only = re.sub(r"\D", "", numbers)
                if len(num_only) == 4:
                    result["å½“ã›ã‚“ç•ªå·"] = num_only
                    result["digit1"] = int(num_only[0])
                    result["digit2"] = int(num_only[1])
                    result["digit3"] = int(num_only[2])
                    result["digit4"] = int(num_only[3])
            
            elif label == "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ" and len(tds) >= 2:
                result["straight_payout"] = parse_payout_amount(tds[1].get_text(strip=True))
            
            elif label == "ãƒœãƒƒã‚¯ã‚¹" and len(tds) >= 2:
                result["box_payout"] = parse_payout_amount(tds[1].get_text(strip=True))
            
            elif label == "ã‚»ãƒƒãƒˆï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆï¼‰" and len(tds) >= 2:
                result["set_straight_payout"] = parse_payout_amount(tds[1].get_text(strip=True))
            
            elif label == "ã‚»ãƒƒãƒˆï¼ˆãƒœãƒƒã‚¯ã‚¹ï¼‰" and len(tds) >= 2:
                result["set_box_payout"] = parse_payout_amount(tds[1].get_text(strip=True))
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿è¿½åŠ 
        if result["å›å·"] and result["å½“ã›ã‚“ç•ªå·"]:
            results.append(result)
    
    return results


def scrape_month(url):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°ï¼ˆé…å½“é‡‘ãªã—ç‰ˆï¼‰"""
    session = requests.Session()
    results = scrape_month_with_payouts(url, session)
    # é…å½“é‡‘ã‚«ãƒ©ãƒ ã‚’é™¤å»ã—ã¦è¿”ã™
    for r in results:
        for key in ["straight_payout", "box_payout", "set_straight_payout", "set_box_payout"]:
            r.pop(key, None)
    return results


def get_max_round_from_past_page(session: requests.Session) -> Optional[int]:
    """numbers4_past ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€å¤§ã®å›å·(æœ«å°¾)ã‚’æ¨å®šã—ã¦è¿”ã™ã€‚
    ä¾‹: href=".../numbers4_detail/6541-6546/" â†’ 6546 ã‚’æŠ½å‡º
    å–å¾—ã«å¤±æ•—ã—ãŸã‚‰ None ã‚’è¿”ã™ã€‚
    """
    try:
        r = session.get(PAST_URL, headers=HEADERS, timeout=20)
        r.encoding = "utf-8"
        soup = BeautifulSoup(r.text, "lxml")
        links = [a.get("href", "") for a in soup.select("a[href]")]
        max_end = 0
        pat = re.compile(r"/numbers4_detail/(\d{4})-(\d{4})/?$")
        for href in links:
            m = pat.search(href)
            if m:
                end = int(m.group(2))
                if end > max_end:
                    max_end = end
        return max_end or None
    except Exception:
        return None


def get_max_round_from_current_month(session: requests.Session) -> Optional[int]:
    """ç¾åœ¨æœˆã®ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®å›å·ã‚’å–å¾—ã™ã‚‹"""
    try:
        # æœ€æ–°ã®æœˆï¼ˆ202509, 202510ãªã©ï¼‰ã‚’é€†é †ã§ãƒã‚§ãƒƒã‚¯
        for month in reversed(MONTHS):
            url = BASE_URL + month + "/"
            r = session.get(url, headers=HEADERS, timeout=20)
            if r.status_code != 200:
                continue
                
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "lxml")
            
            rows = soup.select("table.tblType02.tblNumberGuid tr")
            max_round = 0
            
            round_no = None
            for row in rows:
                th = row.find("th")
                if not th:
                    continue
                    
                label = th.get_text(strip=True)
                if label == "å›å·":
                    round_text = row.find_all("th")[1].get_text(strip=True) if len(row.find_all("th")) > 1 else None
                    if round_text:
                        round_num = to_round_int(round_text)
                        if round_num > max_round:
                            max_round = round_num
            
            if max_round > 0:
                return max_round
        
        return None
    except Exception:
        return None


def build_detail_urls(start_round: int, end_round: int) -> list[str]:
    """0001-0020, 0021-0040 ... ã®ã‚ˆã†ãªç¯„å›²URLã‚’ç”Ÿæˆ"""
    urls = []
    s = max(1, start_round)
    e = max(s, end_round)
    for st in range(s, e + 1, 20):
        en = min(st + 19, e)
        urls.append(f"{DETAIL_BASE}{st:04d}-{en:04d}/")
    return urls


def scrape_detail_page(url: str, session: requests.Session) -> list[dict]:
    """è©³ç´°ãƒšãƒ¼ã‚¸(ç¯„å›²ãƒšãƒ¼ã‚¸)ã‚’1ãƒšãƒ¼ã‚¸åˆ†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ã¦çµæœã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    å¯¾è±¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ table.tblType02.tblNumbers4
    åˆ—: å›å· / æŠ½ã›ã‚“æ—¥ / ãƒŠãƒ³ãƒãƒ¼ã‚º4
    """
    res = session.get(url, headers=HEADERS, timeout=20)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "lxml")

    results: list[dict] = []
    table = soup.select_one("table.tblType02.tblNumbers4")
    if not table:
        return results

    for tr in table.select("tr"):
        tds = tr.find_all("td")
        if len(tds) != 3:
            continue  # ãƒ˜ãƒƒãƒ€è¡Œãªã©ã‚’ã‚¹ã‚­ãƒƒãƒ—
        round_label = tds[0].get_text(strip=True)
        date = tds[1].get_text(strip=True)
        numbers = tds[2].get_text(strip=True)

        # æ•°å­—4æ¡ã ã‘ã‚’æŠ½å‡º(ã‚¹ãƒšãƒ¼ã‚¹ã‚„å…¨è§’å¯¾ç­–)
        num_only = re.sub(r"\D", "", numbers)
        if len(num_only) != 4:
            continue

        result = {
            "å›å·": round_label,
            "æŠ½ã›ã‚“æ—¥": date,
            "å½“ã›ã‚“ç•ªå·": num_only,
            "digit1": int(num_only[0]),
            "digit2": int(num_only[1]),
            "digit3": int(num_only[2]),
            "digit4": int(num_only[3]),
        }
        results.append(result)

    return results


def to_round_int(label: str) -> int:
    # ä¾‹: "ç¬¬0001å›" â†’ 1
    m = re.search(r"(\d+)", label)
    return int(m.group(1)) if m else -1


def get_month_from_date(date_str: str) -> Optional[str]:
    """æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰æœˆã‚’æŠ½å‡º (YYYY/MM/DD â†’ YYYYMM)"""
    try:
        match = re.match(r"(\d{4})/(\d{2})/\d{2}", date_str)
        if match:
            return f"{match.group(1)}{match.group(2)}"
    except Exception:
        pass
    return None


def collect_payouts_from_months(
    session: requests.Session,
    target_rounds: Optional[set] = None,
    months: Optional[List[str]] = None
) -> Dict[int, Dict]:
    """æœˆæ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰é…å½“é‡‘æƒ…å ±ã‚’åé›†
    
    Args:
        session: requestsã‚»ãƒƒã‚·ãƒ§ãƒ³
        target_rounds: å–å¾—å¯¾è±¡ã®å›å·ã‚»ãƒƒãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ã¦å–å¾—ï¼‰
        months: å¯¾è±¡æœˆã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯MONTHSã‚’ä½¿ç”¨ï¼‰
    
    Returns:
        å›å·ã‚’ã‚­ãƒ¼ã¨ã—ãŸé…å½“é‡‘æƒ…å ±ã®è¾æ›¸
    """
    if months is None:
        months = MONTHS
    
    payouts = {}
    
    for month in months:
        url = BASE_URL + month + "/"
        try:
            results = scrape_month_with_payouts(url, session)
            for r in results:
                round_num = to_round_int(r.get("å›å·", ""))
                if round_num > 0:
                    if target_rounds is None or round_num in target_rounds:
                        payouts[round_num] = {
                            "straight_payout": r.get("straight_payout"),
                            "box_payout": r.get("box_payout"),
                            "set_straight_payout": r.get("set_straight_payout"),
                            "set_box_payout": r.get("set_box_payout"),
                        }
            time.sleep(0.3)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        except Exception as e:
            print(f"âš ï¸ æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {url} : {e}")
    
    return payouts


def fill_missing_payouts(df: pd.DataFrame, session: requests.Session) -> pd.DataFrame:
    """é…å½“é‡‘æƒ…å ±ãŒæ¬ æã—ã¦ã„ã‚‹è¡Œã‚’è£œå®Œ
    
    Args:
        df: å…ƒã®DataFrame
        session: requestsã‚»ãƒƒã‚·ãƒ§ãƒ³
    
    Returns:
        è£œå®Œå¾Œã®DataFrame
    """
    # é…å½“é‡‘ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ 
    payout_cols = ["straight_payout", "box_payout", "set_straight_payout", "set_box_payout"]
    for col in payout_cols:
        if col not in df.columns:
            df[col] = None
    
    # é…å½“é‡‘ãŒæ¬ æã—ã¦ã„ã‚‹å›å·ã‚’ç‰¹å®š
    df["__round_int__"] = df["å›å·"].astype(str).apply(to_round_int)
    
    missing_mask = df[payout_cols].isna().all(axis=1)
    missing_rounds = set(df.loc[missing_mask, "__round_int__"].tolist())
    
    if not missing_rounds:
        print("é…å½“é‡‘æƒ…å ±ã®æ¬ æã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        df.drop(columns=["__round_int__"], inplace=True)
        return df
    
    print(f"é…å½“é‡‘ãŒæ¬ æã—ã¦ã„ã‚‹å›å·: {len(missing_rounds)}ä»¶")
    
    # æ¬ æã—ã¦ã„ã‚‹å›å·ã®æ—¥ä»˜ã‹ã‚‰ã€å¿…è¦ãªæœˆã‚’ç‰¹å®š
    months_needed = set()
    for idx, row in df[missing_mask].iterrows():
        month = get_month_from_date(str(row.get("æŠ½ã›ã‚“æ—¥", "")))
        if month:
            months_needed.add(month)
    
    # å¿…è¦ãªæœˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print(f"å–å¾—å¯¾è±¡æœˆ: {sorted(months_needed)}")
    payouts = collect_payouts_from_months(session, missing_rounds, sorted(months_needed))
    
    # æ¬ æã‚’è£œå®Œ
    filled_count = 0
    for idx, row in df.iterrows():
        round_num = row["__round_int__"]
        if round_num in payouts:
            payout_data = payouts[round_num]
            for col in payout_cols:
                if pd.isna(df.at[idx, col]) and payout_data.get(col) is not None:
                    df.at[idx, col] = payout_data[col]
                    filled_count += 1
    
    print(f"è£œå®Œå®Œäº†: {filled_count}ä»¶ã®é…å½“é‡‘ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ")
    
    df.drop(columns=["__round_int__"], inplace=True)
    return df


# CSVåˆ—ã®å®šç¾©ï¼ˆæ–°ä»•æ§˜: 10åˆ—ï¼‰
CSV_COLUMNS = [
    "å›å·", "æŠ½ã›ã‚“æ—¥", "å½“ã›ã‚“ç•ªå·", 
    "digit1", "digit2", "digit3", "digit4",
    "straight_payout", "box_payout", "set_straight_payout", "set_box_payout"
]


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Scrape Numbers4 back numbers from Rakuten Lottery site")
    parser.add_argument("--start", type=int, default=1, help="Start round number (inclusive), default=1")
    parser.add_argument("--end", type=int, default=0, help="End round number (inclusive); when 0, auto-detect from past page")
    parser.add_argument("--append", action="store_true", help="Append only new results to existing numbers4_results.csv if present")
    parser.add_argument("--fill-payouts", action="store_true", help="Fill missing payout data in existing CSV")
    parser.add_argument("--with-payouts", action="store_true", help="Include payout data when scraping (slower)")
    args = parser.parse_args()

    session = requests.Session()

    # é…å½“é‡‘è£œå®Œãƒ¢ãƒ¼ãƒ‰
    if args.fill_payouts:
        if not os.path.exists("numbers4_results.csv"):
            print("ã‚¨ãƒ©ãƒ¼: numbers4_results.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            exit(1)
        
        print("é…å½“é‡‘æƒ…å ±ã®è£œå®Œã‚’é–‹å§‹...")
        df = pd.read_csv("numbers4_results.csv", encoding="utf-8-sig")
        df = fill_missing_payouts(df, session)
        
        # å‡ºåŠ›ã‚«ãƒ©ãƒ ã‚’æ­£è¦åŒ–
        for col in CSV_COLUMNS:
            if col not in df.columns:
                df[col] = None
        
        df[CSV_COLUMNS].to_csv("numbers4_results.csv", index=False, encoding="utf-8-sig")
        print("âœ… é…å½“é‡‘æƒ…å ±ã®è£œå®Œå®Œäº† â†’ numbers4_results.csv")
        exit(0)

    # 1) æœ€æ–°ã®å›å·(æœ«å°¾)ã‚’è‡ªå‹•æ¤œå‡ºã€‚è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™ã€‚
    end_from_past = get_max_round_from_past_page(session)
    end_from_current = get_max_round_from_current_month(session)
    
    # ã‚ˆã‚Šå¤§ããªå€¤ã‚’ä½¿ç”¨ï¼ˆç¾åœ¨æœˆã®æ–¹ãŒé€šå¸¸æœ€æ–°ï¼‰
    end_auto = max(filter(None, [end_from_past, end_from_current]), default=6546)
    
    print(f"éå»ãƒšãƒ¼ã‚¸ã‹ã‚‰æ¤œå‡º: ç¬¬{end_from_past or 0:04d}å›")
    print(f"ç¾åœ¨æœˆã‹ã‚‰æ¤œå‡º: ç¬¬{end_from_current or 0:04d}å›") 
    print(f"ä½¿ç”¨ã™ã‚‹æœ€æ–°å›å·: ç¬¬{end_auto:04d}å›")
    
    start_round = max(1, args.start)
    end_round = args.end if args.end and args.end >= start_round else end_auto
    
    # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜CSVãŒã‚ã‚Œã°æœ€å¾Œã®å›å·ã‚’æ¤œå‡ºã—ã€ãã®æ¬¡ã‹ã‚‰å–å¾—ã™ã‚‹
    existing_df = None
    if args.append and os.path.exists("numbers4_results.csv"):
        try:
            existing_df = pd.read_csv("numbers4_results.csv", encoding="utf-8-sig")
            if not existing_df.empty and "å›å·" in existing_df.columns:
                existing_df["__round_int__"] = existing_df["å›å·"].astype(str).apply(to_round_int)
                max_saved = int(existing_df["__round_int__"].max())
                if max_saved >= end_round:
                    print(f"æ—¢ã«ç¬¬{max_saved:04d}å›ã¾ã§å–å¾—æ¸ˆã¿ã§ã™ã€‚è¿½åŠ å–å¾—ã¯ä¸è¦ã§ã™ã€‚")
                    exit(0)
                start_round = max(start_round, max_saved + 1)
                print(f"è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜CSVã¯ç¬¬{max_saved:04d}å›ã¾ã§ã€‚æ–°è¦å–å¾—ã¯ç¬¬{start_round:04d}å›ï½ç¬¬{end_round:04d}å›ã«ãªã‚Šã¾ã™ã€‚")
        except Exception as e:
            print(f"æ—¢å­˜CSVèª­ã¿è¾¼ã¿å¤±æ•—: {e} â€” ãƒ•ãƒ«å–å¾—ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")

    detail_urls = build_detail_urls(start_round, end_round)

    print(f"å–å¾—ç¯„å›²: ç¬¬{start_round:04d}å›ï½ç¬¬{end_round:04d}å› (å…¨{len(detail_urls)}ãƒšãƒ¼ã‚¸)")

    all_results: List[Dict] = []

    # 2) ç¯„å›²ãƒšãƒ¼ã‚¸ã‚’é †ã«ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆå½“é¸ç•ªå·ã®ã¿å–å¾—ï¼‰
    for i, url in enumerate(detail_urls, 1):
        print(f"[{i}/{len(detail_urls)}] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {url}")
        try:
            page_results = scrape_detail_page(url, session)
            all_results.extend(page_results)
        except Exception as e:
            print(f"âš ï¸ å¤±æ•—: {url} : {e}")
        time.sleep(0.3)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®ã‚¦ã‚§ã‚¤ãƒˆ

    # 3) æœˆæ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰é…å½“é‡‘æƒ…å ±ã‚’å–å¾—
    print("\né…å½“é‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
    
    # å¯¾è±¡å›å·ã‚’ç‰¹å®š
    target_rounds = set()
    for r in all_results:
        round_num = to_round_int(r.get("å›å·", ""))
        if start_round <= round_num <= end_round:
            target_rounds.add(round_num)
    
    # æœˆæ¬¡ãƒšãƒ¼ã‚¸ã‹ã‚‰é…å½“é‡‘ã‚’åé›†ï¼ˆwith-payoutsã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¾ãŸã¯è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    if args.with_payouts or args.append:
        payouts = collect_payouts_from_months(session, target_rounds)
        
        # é…å½“é‡‘æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
        for r in all_results:
            round_num = to_round_int(r.get("å›å·", ""))
            if round_num in payouts:
                r.update(payouts[round_num])
        
        print(f"é…å½“é‡‘æƒ…å ±ã‚’å–å¾—: {len(payouts)}ä»¶")
    else:
        print("é…å½“é‡‘æƒ…å ±ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ--with-payouts ã§å–å¾—å¯èƒ½ï¼‰")

    # 4) é‡è¤‡é™¤å»ã¨ä¸¦ã³æ›¿ãˆ
    for r in all_results:
        if 'round_int' not in r:
            r["round_int"] = to_round_int(r.get("å›å·", ""))

    df_new = pd.DataFrame(all_results)
    
    # é‡è¤‡é™¤å»ï¼ˆåŒã˜å›å·ã®å ´åˆã¯å¾Œã®æ–¹ã‚’æ®‹ã™ï¼‰
    if not df_new.empty:
        df_new.drop_duplicates(subset=["round_int"], keep="last", inplace=True)
    
    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã€æ—¢å­˜CSVã‚’ãã®ã¾ã¾å‡ºåŠ›
    if df_new.empty:
        print("æ–°è¦ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜CSVã¯å¤‰æ›´ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        exit(0)
    
    # 5) çµåˆå‡¦ç†: è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã‹å¦ã‹ã§æŒ™å‹•ã‚’å¤‰ãˆã‚‹
    if args.append and existing_df is not None:
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚’æ­£è¦åŒ–
        for col in CSV_COLUMNS:
            if col not in existing_df.columns:
                existing_df[col] = None
        
        existing_norm = existing_df[CSV_COLUMNS].copy()
        existing_norm["round_int"] = existing_df["å›å·"].astype(str).apply(to_round_int)

        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚‚æ­£è¦åŒ–
        for col in CSV_COLUMNS:
            if col not in df_new.columns:
                df_new[col] = None

        combined = pd.concat([existing_norm, df_new], ignore_index=True)
        combined.sort_values(["round_int", "æŠ½ã›ã‚“æ—¥"], inplace=True)
        combined.drop_duplicates(subset=["round_int"], keep="last", inplace=True)
        out_df = combined[CSV_COLUMNS]
    else:
        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚’æ­£è¦åŒ–
        for col in CSV_COLUMNS:
            if col not in df_new.columns:
                df_new[col] = None
        
        df_new.sort_values(["round_int", "æŠ½ã›ã‚“æ—¥"], inplace=True)
        df_new.drop_duplicates(subset=["round_int"], keep="last", inplace=True)
        out_df = df_new[CSV_COLUMNS]

    out_df.to_csv("numbers4_results.csv", index=False, encoding="utf-8-sig")
    print(f"\nâœ… ä¿å­˜å®Œäº† â†’ numbers4_results.csv ({len(out_df)}ä»¶)")
    
    # é…å½“é‡‘ã®çµ±è¨ˆã‚’è¡¨ç¤º
    payout_cols = ["straight_payout", "box_payout", "set_straight_payout", "set_box_payout"]
    filled = out_df[payout_cols].notna().all(axis=1).sum()
    total = len(out_df)
    print(f"ğŸ“Š é…å½“é‡‘æƒ…å ±: {filled}/{total}ä»¶ ({filled/total*100:.1f}%)")

